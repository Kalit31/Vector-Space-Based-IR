{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "test_queries.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kalit31/IR-Assignment/blob/main/test_queries.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RsvvrU0DLkQW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "101bc54f-4cd3-4bd6-f6d9-aa58a053ce8f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYEcnQWNj8eN",
        "outputId": "7b03fe03-38ff-4958-fcbc-d7b93f55419f"
      },
      "source": [
        "!pip install jsonpickle"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: jsonpickle in /usr/local/lib/python3.7/dist-packages (2.0.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from jsonpickle) (3.10.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->jsonpickle) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->jsonpickle) (3.7.4.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GoAmFS_fXM9c",
        "outputId": "6427bd98-42a2-4598-8a69-d689582fd815"
      },
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import numpy as np\n",
        "import json\n",
        "import jsonpickle\n",
        "import string\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63RWy43geqsC"
      },
      "source": [
        "'''\n",
        "  Document class structure:\n",
        "  id:  doc ID\n",
        "  doc_name: document title\n",
        "  url: document url\n",
        "  tokens: document text splitted into tokens\n",
        "  tf: term frequency vector for the document\n",
        "'''\n",
        "\n",
        "class document:\n",
        "  def __init__(self,tag,id):\n",
        "    self.id = id\n",
        "    self.doc_name = tag[\"title\"]\n",
        "    self.url = tag[\"url\"]\n",
        "    self.tokens=clean_text(tag.get_text())\n",
        "    self.tf = np.zeros((len(vocab),1))\n",
        "  \n",
        "  def create(self):\n",
        "    for token in self.tokens:\n",
        "      token_id = rev_vocab[token].id;\n",
        "      self.tf[token_id]=self.tf[token_id]+1\n",
        "      if(len(inv_index[token])!=0 and inv_index[token][-1]==self.id):\n",
        "        continue\n",
        "      inv_index[token].append(self.id)"
      ],
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUq5FvH5eqzj"
      },
      "source": [
        "'''\n",
        "  Term class structure:\n",
        "  id: id assigned to word\n",
        "  word: original word\n",
        "'''\n",
        "\n",
        "class term:\n",
        "  def __init__(self,id,word):\n",
        "    self.id=id\n",
        "    self.word=word\n",
        "    "
      ],
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMkBNG-NYT7j"
      },
      "source": [
        "def openFile(filePath):\n",
        "  f=open(filePath)\n",
        "  thawed=json.load(f)\n",
        "  return jsonpickle.decode(thawed)"
      ],
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijAh8tlTYT-I"
      },
      "source": [
        "all_docs=openFile('/content/drive/MyDrive/IR_Files/documents.json')\n",
        "vocab=openFile('/content/drive/MyDrive/IR_Files/vocabulary.json')\n",
        "rev_vocab=openFile('/content/drive/MyDrive/IR_Files/reverse-vocabulary.json')\n",
        "inv_index=openFile('/content/drive/MyDrive/IR_Files/inverted-index.json')"
      ],
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7Tj8zpIf_xj"
      },
      "source": [
        "def clean_text(file_text):\n",
        "  '''\n",
        "    Takes a text as input and returns a list of splitted tokens, excluding punctuations\n",
        "\n",
        "    eg: s='Good muffins cost $3.88\\nin New York.  Please buy me two of them.\\n\\n Thanks.'\n",
        "        returns ['Good', 'muffins', 'cost', '3.88', 'in', 'New', 'York','Please', 'buy', 'me', 'two', 'of', 'them', 'Thanks']\n",
        "  '''\n",
        "\n",
        "  #Split text into tokens\n",
        "  tokens=nltk.tokenize.word_tokenize(file_text)\n",
        "  final_tokens=[]\n",
        "  for token in tokens:\n",
        "    # Add into final_tokens after lower casing the token if it is not a punctuation symbol\n",
        "    if(token not in string.punctuation):\n",
        "      token=token.lower()\n",
        "      final_tokens.append(token)\n",
        "  return final_tokens"
      ],
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3DIw4CFXTFa"
      },
      "source": [
        "'''\n",
        "  Query class structure:\n",
        "  query_tokens:  tokens in the query text\n",
        "  query_vector: document title\n",
        "'''\n",
        "\n",
        "class query:\n",
        "  def __init__(self,phrase):\n",
        "    self.query_tokens = clean_text(phrase)\n",
        "    self.query_vector=np.zeros((len(vocab),1))\n",
        "    self.get_query_vector()\n",
        "\n",
        "  def get_query_vector(self):\n",
        "    query_tokens=self.query_tokens\n",
        "    for token in query_tokens:\n",
        "      if(token not in rev_vocab.keys()):\n",
        "        continue\n",
        "      token_id=rev_vocab[token].id\n",
        "      self.query_vector[token_id]+=1\n"
      ],
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkY2gO6kYYjG"
      },
      "source": [
        "def calc_idf():\n",
        "  '''\n",
        "    For each term present in the vocabulary, the idf score is calculated \n",
        "    using the formula: idf = (total number of documents)/(number of documents in which the term is present)\n",
        "  '''\n",
        "  idf_vector = np.zeros((len(vocab),1))\n",
        "  N=len(all_docs)\n",
        "  for i in range(len(vocab)):\n",
        "    idf_vector[i] = N/len(inv_index[vocab[str(i)].word])\n",
        "\n",
        "  idf_vector = np.log10(idf_vector)\n",
        "  return idf_vector"
      ],
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sp9jvjHTlB7N"
      },
      "source": [
        "idf_vector = calc_idf()"
      ],
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpXHYcHull2g"
      },
      "source": [
        "def calc_i(vector):\n",
        "  return np.maximum(1+np.log10(vector),np.zeros(vector.shape) ) "
      ],
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5_hznwIYcoM"
      },
      "source": [
        "def cal_lnc_ltc(doc_vector,query_vector):\n",
        "  '''\n",
        "  calc_i\n",
        "  normalise\n",
        "\n",
        "  calc_i\n",
        "  *idf_vector\n",
        "  normalise\n",
        "\n",
        "  dot\n",
        "  '''\n",
        "  l_doc_vector = calc_i(doc_vector)\n",
        "  c_doc_vector = l_doc_vector/np.linalg.norm(l_doc_vector)\n",
        " \n",
        "  l_query_vector=calc_i(query_vector)\n",
        "  t_query_vector=np.multiply(l_query_vector,idf_vector)\n",
        "  c_query_vector=t_query_vector/np.linalg.norm(t_query_vector)\n",
        " \n",
        "  score=np.dot(c_doc_vector.reshape(c_doc_vector.shape[0]),c_query_vector.reshape(c_query_vector.shape[0]))\n",
        "  return score"
      ],
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_mUPoqDYfeo"
      },
      "source": [
        "q1 = query('Ocean')"
      ],
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzV9sX2_YkES"
      },
      "source": [
        "def ret_ranked_docs():\n",
        "  '''\n",
        "    Retrieves the top 10 document with respect to the lnc.ltc scoring scheme\n",
        "  '''\n",
        "  \n",
        "  sorted_docs=[]\n",
        "  for doc in all_docs.values():\n",
        "    score = cal_lnc_ltc(doc.tf,q1.query_vector)\n",
        "    sorted_docs.append([score, doc.id])\n",
        "  sorted_docs.sort(reverse=True)\n",
        "  retrieve_cnt = min(len(all_docs),10)\n",
        "  return sorted_docs[:retrieve_cnt]"
      ],
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzeXinamgETC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4866185-3f39-4d17-81f2-00ff97cacf79"
      },
      "source": [
        "ret_ranked_docs()"
      ],
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: RuntimeWarning: divide by zero encountered in log10\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0.04984691878042628, 0],\n",
              " [0.0, 56],\n",
              " [0.0, 55],\n",
              " [0.0, 54],\n",
              " [0.0, 53],\n",
              " [0.0, 52],\n",
              " [0.0, 51],\n",
              " [0.0, 50],\n",
              " [0.0, 49],\n",
              " [0.0, 48]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 154
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvH8lDnCnOuD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}