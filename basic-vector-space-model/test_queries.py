# -*- coding: utf-8 -*-
"""test_queries.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1u4iIs1v2_VBWfgilHMmZL0yre1gNovD1
"""

#from google.colab import drive
#drive.mount('/content/drive')

import nltk
from nltk.tokenize import word_tokenize
import numpy as np
import string
import pickle
nltk.download('punkt')
from tabulate import tabulate
import warnings
warnings.filterwarnings("ignore")


'''
  Document class structure:
  id:  doc ID
  doc_name: document title
  url: document url
  tf: term frequency vector for the document
'''
class document:
  def __init__(self,tag,id):
    self.id = id
    self.doc_name = tag["title"]
    self.url = tag["url"]
    self.tf = np.zeros((len(vocabulary),1))

      
'''
  Term class structure:
  id: id assigned to word
  word: original word
'''
class term:
  def __init__(self,id,word):
    self.id=id
    self.word=word

def openFile(filePath):
    fp = open(filePath, "rb")
    data = pickle.load(fp)
    fp.close()
    return data
  
all_docs=openFile('documents')
vocabulary=openFile('vocabulary')
reverse_vocabulary=openFile('reverse-vocabulary')
inverted_index=openFile('inverted-index')


def clean_text(file_text):
  '''
    Takes a text as input and returns a list of splitted tokens, excluding punctuations

    eg: s='Good muffins cost $3.88\nin New York.  Please buy me two of them.\n\n Thanks.'
        returns ['Good', 'muffins', 'cost', '3.88', 'in', 'New', 'York','Please', 'buy', 'me', 'two', 'of', 'them', 'Thanks']
  '''

  #Split text into tokens
  tokens=nltk.tokenize.word_tokenize(file_text)
  final_tokens=[]
  for token in tokens:
    # Add into final_tokens after lower casing the token if it is not a punctuation symbol
    if(token not in string.punctuation):
      token=token.lower()
      final_tokens.append(token)
  return final_tokens

'''
  Query class structure:
  query_tokens:  tokens in the query text
  query_vector: document title
'''
class query:
  def __init__(self,phrase):
    self.query_tokens = clean_text(phrase)
    self.query_vector=np.zeros((len(vocabulary),1))
    self.get_query_vector()

  def get_query_vector(self):
    query_tokens=self.query_tokens
    for token in query_tokens:
      if(token not in reverse_vocabulary.keys()):
        continue
      token_id=reverse_vocabulary[token].id
      self.query_vector[token_id]+=1

def calc_idf():
  '''
    For each term present in the vocabulary, the idf score is calculated 
    using the formula: idf = (total number of documents)/(number of documents in which the term is present)
  '''
  idf_vector = np.zeros((len(vocabulary),1))
  N=len(all_docs)
  for i in range(len(vocabulary)):
    idf_vector[i] = N/len(inverted_index[vocabulary[i].word])

  idf_vector = np.log10(idf_vector)
  return idf_vector

idf_vector = calc_idf()

def calc_i(vector):
  return np.maximum(1+np.log10(vector),np.zeros(vector.shape) )

def cal_lnc_ltc(doc_vector,query_vector):
  '''
  Computes lnc.ltc score for the given document and query
  '''
  l_doc_vector = calc_i(doc_vector)
  c_doc_vector = l_doc_vector/np.linalg.norm(l_doc_vector)
  l_query_vector=calc_i(query_vector)
  t_query_vector=np.multiply(l_query_vector,idf_vector)
  norm=np.linalg.norm(t_query_vector)
  c_query_vector=t_query_vector
  if norm !=0:
    c_query_vector=t_query_vector/norm
  score=np.dot(c_doc_vector.reshape(c_doc_vector.shape[0]),c_query_vector.reshape(c_query_vector.shape[0]))
  return score


def retrieve_ranked_docs(q):
  '''
    Retrieves the top 10 document with respect to the score calculated by cal_lnc_ltc
  '''
  sorted_docs=[]
  for doc in all_docs.values():
    score = cal_lnc_ltc(doc.tf,q.query_vector)
    sorted_docs.append([doc.id, all_docs[doc.id].doc_name,score])
  sorted_docs.sort(reverse=True,key=lambda x:x[2])
  retrieve_cnt = min(len(all_docs),10)
  return sorted_docs[:retrieve_cnt]

def main():
  while(True):
    q = input("\nInput query(Ctrl-C to exit): ")
    queryObj = query(q) 
    print("Retrieving relevant documents...")
    results = retrieve_ranked_docs(queryObj)
    print(tabulate(results, headers=["Document ID", "Document Name", "Score"]))
  
if __name__ == "__main__":
  main()