{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "index_creation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kalit31/IR-Assignment/blob/main/index_creation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILXJH2cNC02n",
        "outputId": "a1538d8d-1854-4507-edee-317a1299792e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Hwa6zb4RNtp",
        "outputId": "8d8054af-b7c1-4d0d-da8c-bbbf1558be09"
      },
      "source": [
        "pip install beautifulsoup4"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (4.6.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7jjspUY8aDS4",
        "outputId": "c907c71e-e4a8-4be5-811f-b441d15c59c8"
      },
      "source": [
        "!pip install jsonpickle"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: jsonpickle in /usr/local/lib/python3.7/dist-packages (2.0.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from jsonpickle) (3.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->jsonpickle) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->jsonpickle) (3.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxAjEjDsEnTc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbd6e48b-756a-4d14-d913-68d5ebde8b5b"
      },
      "source": [
        "import bs4 as bs \n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import numpy as np\n",
        "import json\n",
        "import jsonpickle\n",
        "import string\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHAo8Sl_PRND"
      },
      "source": [
        "'''\n",
        "Intialize data structures\n",
        "\n",
        "vocab{\n",
        "  0 : term0  //bits\n",
        "  1: term1\n",
        "}\n",
        "\n",
        "rev_vocab{\n",
        "  term0.word :term0\n",
        "  term1.word :term1\n",
        "}\n",
        "\n",
        "all_docs: stores all the documents found in the file\n",
        "\n",
        "inv_index: stores the inverted index for the corpus\n",
        "\n",
        "'''\n",
        "\n",
        "vocab = {}\n",
        "rev_vocab ={}\n",
        "all_docs = {}\n",
        "inv_index = {}"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmnRSXnZbjBz"
      },
      "source": [
        "def clean_text(file_text):\n",
        "  '''\n",
        "    Takes a text as input and returns a list of splitted tokens, excluding punctuations\n",
        "\n",
        "    eg: s='Good muffins cost $3.88\\nin New York.  Please buy me two of them.\\n\\n Thanks.'\n",
        "        returns ['Good', 'muffins', 'cost', '3.88', 'in', 'New', 'York','Please', 'buy', 'me', 'two', 'of', 'them', 'Thanks']\n",
        "  '''\n",
        "\n",
        "  #Split text into tokens\n",
        "  tokens=nltk.tokenize.word_tokenize(file_text)\n",
        "  final_tokens=[]\n",
        "  for token in tokens:\n",
        "    # Add into final_tokens after lower casing the token if it is not a punctuation symbol\n",
        "    if(token not in string.punctuation):\n",
        "      token=token.lower()\n",
        "      final_tokens.append(token)\n",
        "  return final_tokens"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IVllZjBJbp4"
      },
      "source": [
        "'''\n",
        "  Document class structure:\n",
        "  id:  doc ID\n",
        "  doc_name: document title\n",
        "  url: document url\n",
        "  tokens: document text splitted into tokens\n",
        "  tf: term frequency vector for the document\n",
        "'''\n",
        "\n",
        "class document:\n",
        "  def __init__(self,tag,id):\n",
        "    self.id = id\n",
        "    self.doc_name = tag[\"title\"]\n",
        "    self.url = tag[\"url\"]\n",
        "    self.tokens=clean_text(tag.get_text())\n",
        "    self.tf = np.zeros((len(vocab),1))\n",
        "  \n",
        "  def create(self):\n",
        "    for token in self.tokens:\n",
        "      token_id = rev_vocab[token].id;\n",
        "      self.tf[token_id]=self.tf[token_id]+1\n",
        "      if(len(inv_index[token])!=0 and inv_index[token][-1]==self.id):\n",
        "        continue\n",
        "      inv_index[token].append(self.id)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpByvntDZUnA"
      },
      "source": [
        "'''\n",
        "  Term class structure:\n",
        "  id: id assigned to word\n",
        "  word: original word\n",
        "'''\n",
        "\n",
        "class term:\n",
        "  def __init__(self,id,word):\n",
        "    self.id=id\n",
        "    self.word=word\n",
        "    "
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcuiJZO7XXkE"
      },
      "source": [
        "def create_vocab_dicts(doc_text):\n",
        "  '''\n",
        "  creates vocabulary and reverse-vocabulary using tokens tokens returned from clean_text function\n",
        "    vocab{\n",
        "      0 : term0\n",
        "      1: term1\n",
        "    }\n",
        "\n",
        "    rev_vocab{\n",
        "      term0.word :term0\n",
        "      term1.word :term1\n",
        "    }\n",
        "  '''\n",
        "  tokens = clean_text(doc_text)\n",
        "  for token in tokens:\n",
        "    if(token in rev_vocab.keys()): \n",
        "      continue\n",
        "    term_obj=term(len(vocab),token)\n",
        "    vocab[term_obj.id]=term_obj\n",
        "    rev_vocab[term_obj.word]=term_obj\n",
        "    inv_index[term_obj.word]=[]"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2spOT3mCJFj3"
      },
      "source": [
        "def parse_docs(filename):\n",
        "  '''\n",
        "    Takes filename as input, extracts text from it.\n",
        "    Parses the documents found into appropriate objects \n",
        "  '''\n",
        "\n",
        "  file = open(filename, \"r\")\n",
        "  readBytes = 100000\n",
        "  data = file.read(readBytes)\n",
        "\n",
        "  soup = bs.BeautifulSoup(data,'html.parser')\n",
        "  all_doc_tags = soup.find_all('doc')\n",
        "\n",
        "  print(str(len(all_doc_tags))+\" documents found in the file.\")\n",
        "\n",
        "  # Create vocabulary for the corpus\n",
        "  for each_tag in all_doc_tags:\n",
        "    create_vocab_dicts(each_tag.get_text())\n",
        "\n",
        "  #Store each document object\n",
        "  for i,each_tag in enumerate(all_doc_tags):\n",
        "    all_docs[i] = document(each_tag,i)\n",
        "    all_docs[i].create()\n",
        "  "
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FN5wp1fiWg6f"
      },
      "source": [
        "filePath = \"/content/drive/MyDrive/IR_Files/wiki_00\""
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5LjKkVFRGE4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1fd016f-8fd4-43f0-e611-102c1e56e3ce"
      },
      "source": [
        "parse_docs(filePath)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "57 documents found in the file.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEEaX5lLXC4L"
      },
      "source": [
        "#Save files\n",
        "def saveFiles(obj,fileName):\n",
        "  frozen=jsonpickle.encode(obj)\n",
        "  with open(fileName, 'w') as f:\n",
        "    json.dump(frozen,f)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDRfxp6Txjb9"
      },
      "source": [
        "saveFiles(all_docs,'/content/drive/MyDrive/IR_Files/documents.json')\n",
        "saveFiles(vocab,'/content/drive/MyDrive/IR_Files/vocabulary.json')\n",
        "saveFiles(rev_vocab,'/content/drive/MyDrive/IR_Files/reverse-vocabulary.json')\n",
        "saveFiles(inv_index,'/content/drive/MyDrive/IR_Files/inverted-index.json')"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJBAtYNXmhsw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}